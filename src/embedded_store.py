import hashlib
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

def generate_doc_id(file_path):
    """T·∫°o doc_id duy nh·∫•t d·ª±a tr√™n n·ªôi dung file."""
    with open(file_path, "rb") as f:
        data = f.read()
    return hashlib.md5(data).hexdigest()

def create_or_update_vector_db(chunks, file_path, persist_dir="vectorstore"):
    """Upsert vectorstore cho 1 file PDF."""
    doc_id = generate_doc_id(file_path)

    # G·∫Øn doc_id v√†o metadata c·ªßa t·ª´ng chunk
    for c in chunks:
        c.metadata["doc_id"] = doc_id
        c.metadata["source_file"] = file_path  # ti·ªán ƒë·ªÉ trace

    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)

    # N·∫øu doc_id ƒë√£ t·ªìn t·∫°i ‚Üí xo√° tr∆∞·ªõc
    existing_ids = [f"{doc_id}_{i}" for i in range(10_000)]  # s·ªë ƒë·ªß l·ªõn
    vectordb.delete(ids=existing_ids)
    print(f"üóëÔ∏è  Deleted old chunks for doc {file_path} (doc_id={doc_id})")

    # Add chunks m·ªõi
    vectordb.add_documents(chunks)
    print(f"‚úÖ Upserted {len(chunks)} chunks for doc {file_path}")
    return vectordb


def create_or_update_vector_db_from_collection(chunks, collection_name, persist_dir="vectorstore"):
    """
    Upsert vectorstore cho m·ªôt collection (v√≠ d·ª•: blog posts).
    D√πng collection_name l√†m doc_id thay v√¨ hash file content.
    
    Args:
        chunks: List of Document chunks
        collection_name: T√™n collection (d√πng l√†m doc_id)
        persist_dir: Th∆∞ m·ª•c l∆∞u vector database
    """
    doc_id = hashlib.md5(collection_name.encode()).hexdigest()
    
    # G·∫Øn doc_id v√†o metadata c·ªßa t·ª´ng chunk
    for c in chunks:
        c.metadata["doc_id"] = doc_id
        c.metadata["collection"] = collection_name
    
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    
    # N·∫øu doc_id ƒë√£ t·ªìn t·∫°i ‚Üí xo√° tr∆∞·ªõc
    existing_ids = [f"{doc_id}_{i}" for i in range(10_000)]
    vectordb.delete(ids=existing_ids)
    print(f"üóëÔ∏è  Deleted old chunks for collection {collection_name} (doc_id={doc_id})")
    
    # Add chunks m·ªõi
    vectordb.add_documents(chunks)
    print(f"‚úÖ Upserted {len(chunks)} chunks for collection {collection_name}")
    return vectordb


def load_vector_db(persist_dir="vectorstore"):
    """Load l·∫°i vector DB ƒë√£ l∆∞u."""
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    return vectordb